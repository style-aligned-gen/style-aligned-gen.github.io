<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>StyleAlign</title>
<link href="./style.css" rel="stylesheet">
<!-- <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script> -->
</head>

<body>
<div class="content">
  <h1><strong>Style Aligned Image Generation via Shared Attention</strong></h1>
  <p id="authors"><a href="https://amirhertz.github.io/">Amir Hertz<sup>* 1</sup></a> <a href="">Andrey Voynov<sup>* 1</sup></a> <a href="">Shlomi Fruchter<sup>† 1</sup></a> <a href="https://danielcohenor.com/">Daniel Cohen-Or<sup>† 1,2</sup></a><br>
    <br>
    <span style="font-size: 16px"><br>
        <sup>1</sup> Google Research <sup>2</sup> Tel Aviv University <br>
     <sup>*</sup>Indicates Equal Contribution <sup>†</sup>Indicates Equal Advising</span>
        </p>
    <br>
  <img src="./data/teaser2.png" class="teaser-gif" style="width:100%;"><br>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/TODO" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	        <a target="_blank">[Code coming soon]</a>
          </p>
    </font>
    <h3>StyleAligned performs consistent style generation and style transfer with a pretrained diffusion model <u>without any fine-tuning</u>.</h3>
</div>


<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visually compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique designed to establish style alignment among a series of generated images. By employing minimal `attention sharing' during the diffusion process, our method maintains style consistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.</p>
</div>


<div class="content">
    <h2>Problem statement</h2>
    <p>  While proficient in aligning with the textual description of the style, state-of-the-art text-to-image models often create images that diverge significantly in their interpretations of the same stylistic descriptor. Given a style description of ``minimal origami'', standard text-to-image generation (left) outputs images with signifficantly different styles. With our method we make the model generation style-persisten (right).</p>
    <br>
    <img class="summary-img" src="./data/cmp.png" style="width:100%;"> <br>
  </div>

<div class="content">
  <h2>Style Aligned Generation</h2>
  <p> Generation of images with a style aligned to the reference image on the left. In each diffusion denoising step all the images, except the reference, perform a shared self-attention with the reference image.</p>
  <br>
  <img class="summary-img" src="./data/method.jpg" style="width:100%;"> <br>
  <br>
  <p>The target images attends to the reference image by applying AdaIN over their queries and keys using the reference queries and keys. Then, we apply shared attention where the target features are updated by both the target values Vt and the reference values Vr.</p>
</div>


<div class="content">
  <h2>Results</h2>
  <p>Our methods allows to generate style-consistent images with different prompts <b>without any fine-tuning</b>, outperforming personalization baselines. </p>
  <img class="summary-img" src="./data/results/comparison_p.png" style="width:100%;">
  <br>
  <p>This method also works well with style transfer from real image while doesn't require any training or model personalization. </p>
  <img class="summary-img" src="./data/results/reals.jpg" style="width:100%;">
  <br>
  <p>It is also applicable in combination with other methods like ControlNet and Textual Inversion. </p>
  <img class="summary-img" src="./data/results/control_pose.jpeg" style="width:100%;">
  <br>
  <img class="summary-img" src="./data/results/textual_inversion.jpg" style="width:100%;">
</div>


<div class="content">
  <h2>BibTex</h2>
  <code> @article{hertz2023StyleAligned,<br>
  &nbsp;&nbsp;title={Style Aligned Image Generation via Shared Attention},<br>
  &nbsp;&nbsp;author={Hertz, Amir and Voynov, Andrey and Fruchter, Shlomi and Cohen-Or, Daniel},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:TODO},<br>
  &nbsp;&nbsp;year={2023}<br>
  } </code> 
</div>


<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Matan Cohen, Yael Pritch, Yael Vinker, TODO for their valuable inputs and early feedback that contributed to this work. We specially thank Yael Vinker for providing one of here artworks as style reference.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  </p>
</div>
</body>

<br>
<body font-family="Google Sans">
    <h2 style="text-align:center;">Can you guess all of the places? Click on image to know the answer.</h2>
    <p style="text-align:center;">All of the images are generated with StyleAligned, first image serves as the style source.</p>
    <div id="grid-container" class="grid-container">
        <!-- Grid items will be inserted here by script.js -->
    </div>
    <script src="quiz.js"></script>
</body>

</html>